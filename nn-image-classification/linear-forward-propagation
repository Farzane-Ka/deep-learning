def linear_forward(A, W, b):
  
    Z=np.dot(W,A)+b
    cache = (A, W, b)
    
    return Z, cache
#A is the input or the activation input from previous layer
#cache is used in backward propagation when computing dW and db

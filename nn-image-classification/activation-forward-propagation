def linear_activation_forward(A_prev, W, b, activation):
 
    if activation == "sigmoid":
       
        Z, linear_cache =linear_forward(A_prev,W,b)
        A, activation_cache =sigmoid(Z)
    
    elif activation == "relu":
        Z, linear_cache =linear_forward(A_prev,W,b)
        A, activation_cache =relu(Z)
        
    cache = (linear_cache, activation_cache)

    return A, cache
 #linear-cache is a tuple of (A-prev,W,b)
 #activation cache is Z for current layer l

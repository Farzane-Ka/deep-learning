#L layers relu as the activation function and the last layer is sigmoid

def L_model_forward(X, parameters):

    caches = []
    A = X
    L = len(parameters) // 2                  # number of layers in the neural network
    
    # Implementing [LINEAR -> RELU]*(L-1). Add "cache" to the "caches" list.
    for l in range(1, L):
        A_prev = A 
        A, cache=linear_activation_forward(A_prev,parameters['W' + str(l)], parameters['b' + str(l)],activation="relu")
        caches.append(cache)
    
    # Implement LINEAR -> SIGMOID for binary classification 0 or 1
    AL, cache=linear_activation_forward(A,parameters['W' + str(L)], parameters['b' + str(L)],activation="sigmoid")
    caches.append(cache)
          
    return AL, caches
